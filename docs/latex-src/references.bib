@article{
    Rainbow, 
    title={Rainbow: Combining Improvements in Deep Reinforcement Learning},
    volume={32}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/11796}, abstractNote={ &lt;p&gt; The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance. &lt;/p&gt; }, 
    number={1}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
    year={2018},
    month={Apr.}
},
@article{DDQL, title={Deep Reinforcement Learning with Double Q-Learning}, volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10295}, abstractNote={ &lt;p&gt; The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={van Hasselt, Hado and Guez, Arthur and Silver, David}, year={2016}, month={3} },

@ARTICLE{DRLsurv,
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine}, 
  title={Deep Reinforcement Learning: A Brief Survey}, 
  year={2017},
  volume={34},
  number={6},
  pages={26-38},
  doi={10.1109/MSP.2017.2743240}},
  
@article{openaigym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
},
@article{chendeep,
  title={Deep Reinforcement Learning for Flappy Bird. cs229},
  author={Chen, K},
  journal={Stanford. Edu},
  volume={6},
  pages={90132--4},
  year={2015}
},
@article{playingatari,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
},
@article{humanlvl,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@online{geneticFB, 
    author = {S. Susnic},
    title = {Machine Learning Algorithm for Flappy Bird using Neural Network and Genetic Algorithm},
    year = 2017,
    month = August,
    url = {https://www.askforgametask.com/tutorial/machine-learning/machine-learning-algorithm-flappy-bird/},
    urldate = {2022-03-21}
}
@InProceedings{performanceFB,
author="Mishra, Yash
and Kumawat, Vijay
and Selvakumar, K.",
editor="Gani, Abdullah Bin
and Das, Pradip Kumar
and Kharb, Latika
and Chahal, Deepak",
title="Performance Analysis of Flappy Bird Playing Agent Using Neural Network and Genetic Algorithm",
booktitle="Information, Communication and Computing Technology",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="253--265",
abstract="The aim of this paper is to develop and study an artificial intelligence based game-playing agent using genetic algorithm and neural networks. We first create an agent which learns how to optimally play the famous ``Flappy Bird'' game by safely dodging all the barriers and flapping its way through them and then study the effect of changing various parameters like number of neurons on the hidden layer, gravity, speed, gap between trees has on the learning process. The gameplay was divided into two level of difficulty to facilitate study on the learning process. Phaser Framework was used to facilitate HTML5 programming for introducing real-life factors like gravity, collision and Synaptic Neural Network library was used to implement neural network so as to avoid creating a neural network from scratch. Machine Learning Algorithm which we have adopted in this project is based on the concept of Neuro-Evolution and this form of machine learning uses algorithms which can evolve and mature over time such as a genetic algorithm to train artificial neural networks.",
isbn="978-981-15-1384-8"
}

